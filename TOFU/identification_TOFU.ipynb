{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
    "from datasets import load_dataset\n",
    "from itertools import groupby\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "hug_token = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and pipeline\n",
    "model_name_bert = \"distilroberta-base\"\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(model_name_bert)\n",
    "model_bert = AutoModelForMaskedLM.from_pretrained(model_name_bert)\n",
    "fill_mask = pipeline(\"fill-mask\", model=model_bert, tokenizer=tokenizer_bert, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_dataset(\"locuslab/TOFU\", \"forget05\")['train']\n",
    "retain_test_dataset = load_dataset(\"locuslab/TOFU\", \"retain95\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_common_words(sentence, length, threshold=0.9):\n",
    "    # Split sentence into words and track positions\n",
    "    words = sentence.split()\n",
    "    common_words = []\n",
    "\n",
    "    # Get start and end positions for each word\n",
    "    word_start_end_positions = []\n",
    "    start_idx = 0\n",
    "    for word in words:\n",
    "        start_idx = sentence.find(word, start_idx)  # Find word's start index in the sentence\n",
    "        end_idx = start_idx + len(word)             # Calculate end index\n",
    "        word_start_end_positions.append((word, start_idx, end_idx))\n",
    "        start_idx = end_idx                         # Update start_idx for the next search\n",
    "\n",
    "    # Create masked sentences in a batch\n",
    "    masked_sentences = [sentence[:start] + \"<mask>\" + sentence[end:] for _, start, end in word_start_end_positions]\n",
    "\n",
    "    # Get predictions in a batch\n",
    "    all_predictions = fill_mask(masked_sentences, top_k=1)\n",
    "\n",
    "    # Identify non-common words based on predictions\n",
    "    for i, (word, start, end) in enumerate(word_start_end_positions):\n",
    "        # Check if the original word's probability is below the threshold\n",
    "        # common\n",
    "        for pred in all_predictions[i]:\n",
    "            if start>= length and pred['token_str'].strip().lower() == word.lower() and pred['score'] >= threshold:\n",
    "                common_words.append((word, start-length, end-length))\n",
    "\n",
    "    return common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_common_words(sample, threshold=0):\n",
    "    common_words = identify_common_words(sample['question'] +' '+ sample['answer'], len(sample['question']) +1,threshold)\n",
    "    sample['common_words'] = [json.dumps({\"word\": word, \"start\": start, \"end\": end}) for word, start, end in common_words]\n",
    "    return sample\n",
    "\n",
    "# Apply the function to each sample in the dataset and save the modifWied dataset\n",
    "test_dataset = test_dataset.map(lambda x: add_common_words(x))\n",
    "test_dataset.save_to_disk(\"../data/forget05_with_common_words\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
