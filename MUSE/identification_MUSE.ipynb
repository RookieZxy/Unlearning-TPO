{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "hug_token = ''\n",
    "file_path = '../data/books/raw/forget.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    MUSE_dataset = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_bert = \"distilroberta-base\"\n",
    "model_name_bert = \"bert-large-uncased\"\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(model_name_bert)\n",
    "model_bert = AutoModelForMaskedLM.from_pretrained(model_name_bert)\n",
    "fill_mask = pipeline(\"fill-mask\", model=model_bert, tokenizer=tokenizer_bert, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_common_words(sentence, pre_text_len, threshold=0.9):\n",
    "    # Split sentence into words and track positions\n",
    "    words = sentence.split()\n",
    "    common_words = []\n",
    "\n",
    "    # Get start and end positions for each word\n",
    "    word_start_end_positions = []\n",
    "    start_idx = 0\n",
    "    for word in words:\n",
    "        start_idx = sentence.find(word, start_idx)  # Find word's start index in the sentence\n",
    "        end_idx = start_idx + len(word)             # Calculate end index\n",
    "        word_start_end_positions.append((word, start_idx, end_idx))\n",
    "        start_idx = end_idx                         # Update start_idx for the next search\n",
    "\n",
    "    # Create masked sentences in a batch\n",
    "    # masked_sentences = [sentence[:start] + \"<mask>\" + sentence[end:] for _, start, end in word_start_end_positions]\n",
    "    masked_sentences = [sentence[:start] + \"[MASK]\" + sentence[end:] for _, start, end in word_start_end_positions]\n",
    "\n",
    "    # Get predictions in a batch\n",
    "    all_predictions = fill_mask(masked_sentences, top_k=1)\n",
    "\n",
    "    # Identify non-common words based on predictions\n",
    "    for i, (word, start, end) in enumerate(word_start_end_positions):\n",
    "        # Check if the original word's probability is below the threshold\n",
    "        if len(masked_sentences) == 1:\n",
    "            all_predictions = [all_predictions]\n",
    "        for pred in all_predictions[i]:\n",
    "            try:\n",
    "                if pred['token_str'].strip() == word and pred['score'] >= threshold:\n",
    "                    common_words.append((word, start+pre_text_len, end+pre_text_len))\n",
    "            except:\n",
    "                print(pred)\n",
    "                print(pred['token_str'].strip().lower())\n",
    "                print(word.lower())\n",
    "\n",
    "    return common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_common_words = None\n",
    "common_words = []\n",
    "pre_text_len = 0\n",
    "splited_sentence =re.split(r'(\\. |\\n)', MUSE_dataset)\n",
    "\n",
    "for item in tqdm(splited_sentence, desc=\"Processing sentences\"):\n",
    "    # print(item)\n",
    "    output = identify_common_words(item, pre_text_len, threshold=0.9)\n",
    "    common_words += output\n",
    "    if item == '\\n':\n",
    "        pre_text_len += 1\n",
    "    elif item == '. ':\n",
    "        pre_text_len += 2\n",
    "    else:\n",
    "        pre_text_len += len(item)\n",
    "\n",
    "all_common_words = [json.dumps({\"word\": item[0], \"start\": item[1], \"end\": item[2]}) for item in common_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"../data/books/raw/forget_common_words_bert.json\"\n",
    "\n",
    "with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_common_words, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
